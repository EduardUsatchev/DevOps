import os
import pathlib
import openai
import re
import json
import time
import logging
import argparse
import traceback
import asyncio
import ast
from aiofiles import open as aioopen
from tqdm.asyncio import tqdm_asyncio

# --- Configuration & Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("../../../generate_docs_async.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Retrieve API key from environment variable.
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    logger.error("No API key provided. Please set the OPENAI_API_KEY environment variable.")
    raise ValueError("No API key provided. Please set the OPENAI_API_KEY environment variable.")
openai.api_key = api_key

# Allowed file extensions (can be overridden via command-line)
DEFAULT_ALLOWED_EXTENSIONS = {".py", ".json", ".txt", ".md"}

# Cache file for modification times.
CACHE_FILE = "doc_cache.json"
try:
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        cache = json.load(f)
except Exception:
    cache = {}

# Marker used in generated documentation files.
GEN_MARKER = "<!-- Generated by generate_docs.py -->"

# Default base prompt (can be overridden via config file or argument)
DEFAULT_BASE_PROMPT = """
You are a senior software engineer with extensive experience in Python, FastAPI, and microservice architecture.
Your task is to review the provided context about a file or folder and generate professional documentation.
Include the following in your output:
- A summary of the file or folder's purpose and its role in the project.
- A structural overview: list of classes, functions, or files (with brief descriptions or docstrings if available).
- How the file or folder fits into the overall system.
- A section titled "Usage:" describing how the materials are intended to be used.
The context will include both raw content and static analysis details.
Now generate the documentation based on the following context:
"""

# Global variable for token usage tracking.
total_token_usage = 0

# --- Function to Extract Docstrings ---
def extract_docstrings(file_content: str) -> str:
    """
    Extracts all docstrings from the provided file content.
    Returns a concatenated string of all docstrings found.
    """
    docstring_pattern = re.compile(r'("""|\'\'\')(.*?)\1', re.DOTALL)
    matches = docstring_pattern.findall(file_content)
    docstrings = "\n".join(match[1].strip() for match in matches if match[1].strip())
    return docstrings

# --- Asynchronous API Call with Retry ---
async def retry_api_call(context: str, max_tokens: int = 500, max_retries: int = 3, initial_delay: float = 1.0) -> str:
    global total_token_usage
    retries = 0
    delay = initial_delay
    while retries < max_retries:
        try:
            response = await openai.ChatCompletion.acreate(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": BASE_PROMPT},
                    {"role": "user", "content": context}
                ],
                temperature=0.3,
                max_tokens=max_tokens,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0
            )
            usage = response.get("usage", {})
            if usage:
                call_usage = usage.get("total_tokens", 0)
                total_token_usage += call_usage
                logger.debug(f"Call token usage: {call_usage}, Total so far: {total_token_usage}")
            return response.choices[0].message.content.strip()
        except Exception as e:
            retries += 1
            logger.error(f"API call failed (attempt {retries}): {e}")
            logger.debug(traceback.format_exc())
            if retries >= max_retries:
                logger.error("Max retries reached; giving up on this API call.")
                return "Documentation generation failed."
            await asyncio.sleep(delay)
            delay *= 2  # Exponential backoff

# --- Static Analysis: AST-Based Extraction for Python Files ---
def analyze_python_file(file_content: str) -> str:
    """
    Use AST to extract a summary of classes and functions along with their docstrings.
    Returns a string summary that can be added to the prompt.
    """
    try:
        tree = ast.parse(file_content)
    except Exception as e:
        logger.error(f"AST parsing failed: {e}")
        return "No static analysis available."

    analysis = []
    for node in ast.iter_child_nodes(tree):
        if isinstance(node, ast.ClassDef):
            class_summary = f"Class: {node.name}"
            doc = ast.get_docstring(node)
            if doc:
                class_summary += f"\n  Docstring: {doc}"
            methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]
            if methods:
                class_summary += "\n  Methods:"
                for m in methods:
                    method_summary = f"    - {m.name}"
                    m_doc = ast.get_docstring(m)
                    if m_doc:
                        method_summary += f": {m_doc.splitlines()[0]}"
                    class_summary += "\n" + method_summary
            analysis.append(class_summary)
        elif isinstance(node, ast.FunctionDef):
            func_summary = f"Function: {node.name}"
            doc = ast.get_docstring(node)
            if doc:
                func_summary += f": {doc.splitlines()[0]}"
            analysis.append(func_summary)
    if analysis:
        return "\n".join(analysis)
    else:
        return "No classes or functions found."

# --- Caching: Check if File Has Been Modified ---
def file_needs_update(file_path: pathlib.Path) -> bool:
    mod_time = file_path.stat().st_mtime
    cached_time = cache.get(str(file_path))
    if cached_time and cached_time == mod_time:
        return False
    cache[str(file_path)] = mod_time
    return True

def save_cache():
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, indent=4)

# --- README Validation and Update for Folders ---
def validate_or_update_readme(folder_path: pathlib.Path, new_content: str) -> None:
    """
    Checks if a README.md exists in the folder.
    If it exists, verifies whether it contains the generation marker and a "Usage:" section.
    If not, updates (or creates) the README with the new content.
    """
    readme_path = folder_path / "README.md"
    needs_update = True

    if readme_path.exists():
        try:
            with open(readme_path, "r", encoding="utf-8") as f:
                existing_content = f.read()
            # Check if the existing README contains the generation marker and the "Usage:" section.
            if GEN_MARKER in existing_content and "Usage:" in existing_content:
                needs_update = False
        except Exception as e:
            logger.error(f"Failed to read existing README in {folder_path}: {e}")

    if needs_update:
        try:
            with open(readme_path, "w", encoding="utf-8") as f:
                f.write(f"# Documentation for folder: {folder_path.name}\n\n")
                f.write(f"{GEN_MARKER}\n\n")
                f.write(new_content)
            logger.info(f"Updated README for folder {folder_path}")
        except Exception as e:
            logger.error(f"Failed to write README for folder {folder_path}: {e}")

# --- Documentation Generation Functions ---
async def document_file(file_path: pathlib.Path) -> None:
    """Generate documentation for a single file if it needs an update."""
    if file_path.suffix.lower() not in args.allowed_extensions:
        logger.info(f"Skipping non-text file: {file_path}")
        return

    if not file_needs_update(file_path):
        logger.info(f"Skipping unchanged file: {file_path}")
        return

    try:
        async with aioopen(file_path, "r", encoding="utf-8") as f:
            content = await f.read()
    except Exception as e:
        logger.error(f"Failed to read {file_path}: {e}")
        logger.debug(traceback.format_exc())
        return

    if file_path.suffix.lower() == ".py":
        file_type = "Python code file"
        static_analysis = analyze_python_file(content)
        docstrings = extract_docstrings(content)
    else:
        file_type = "Non-code file"
        static_analysis = "N/A"
        docstrings = ""

    context = (
        f"File Path: {file_path}\n"
        f"File Type: {file_type}\n\n"
        f"--- File Excerpt (first 1000 characters) ---\n{content[:1000]}\n\n"
        f"--- Static Analysis ---\n{static_analysis}\n\n"
        f"--- Extracted Docstrings ---\n{docstrings}"
    )
    documentation = await retry_api_call(context)
    readme_path = file_path.parent / f"{file_path.stem}_README.md"
    try:
        async with aioopen(readme_path, "w", encoding="utf-8") as f:
            await f.write(f"# Documentation for {file_path.name}\n\n")
            await f.write(f"{GEN_MARKER}\n\n")
            await f.write(documentation)
        logger.info(f"Generated documentation for {file_path}")
    except Exception as e:
        logger.error(f"Failed to write documentation for {file_path}: {e}")
        logger.debug(traceback.format_exc())

async def document_folder(folder_path: pathlib.Path) -> None:
    """Generate documentation for a folder."""
    files = [f for f in folder_path.iterdir() if f.is_file()]
    subfolders = [d for d in folder_path.iterdir() if d.is_dir()]
    files_list = "\n".join([f"- {f.name}" for f in files])
    subfolders_list = "\n".join([f"- {d.name}" for d in subfolders])

    context = (
        f"Folder Path: {folder_path}\n\n"
        f"Files in folder:\n{files_list}\n\n"
        f"Subfolders:\n{subfolders_list}\n\n"
        "Describe the folder's purpose, its usage, intended audience, and how the materials (including DevOps class material) should be used."
    )
    documentation = await retry_api_call(context)
    # Use the new function to update or validate the README.
    validate_or_update_readme(folder_path, documentation)

async def generate_root_overview(root_path: pathlib.Path) -> None:
    """Generate a root-level architectural overview."""
    context = (
        f"Project Root: {root_path}\n\n"
        "Provide a high-level architectural overview of the entire project. Describe how various folders (including DevOps materials) fit into the project. "
        "Include details on usage, key components, and suggestions for improvements or refactoring."
    )
    documentation = await retry_api_call(context, max_tokens=800)
    overview_path = root_path / "PROJECT_OVERVIEW.md"
    try:
        async with aioopen(overview_path, "w", encoding="utf-8") as f:
            await f.write("# Project Architectural Overview\n\n")
            await f.write(f"{GEN_MARKER}\n\n")
            await f.write(documentation)
        logger.info("Generated root-level project overview.")
    except Exception as e:
        logger.error(f"Failed to write project overview: {e}")
        logger.debug(traceback.format_exc())

# --- Main Iteration Function ---
async def iterate_project(root_dir: pathlib.Path) -> dict:
    """Recursively iterate through all folders and files to generate documentation."""
    summary = {
        "folders_processed": 0,
        "files_processed": 0,
        "files_skipped": 0,
        "errors": 0
    }
    await generate_root_overview(root_dir)

    # Gather all file paths to process (skipping hidden directories and 'venv').
    file_list = []
    for dirpath, dirnames, filenames in os.walk(root_dir):
        current_dir = pathlib.Path(dirpath)
        if any(part.startswith('.') for part in current_dir.parts) or 'venv' in current_dir.parts:
            continue
        summary["folders_processed"] += 1
        for filename in filenames:
            # Skip auto-generated README files and project overview files.
            if filename.endswith("_README.md") or filename in {"README.md", "PROJECT_OVERVIEW.md"}:
                continue
            file_path = current_dir / filename
            file_list.append(file_path)

    logger.info(f"Found {len(file_list)} files to process.")

    tasks = [document_file(fp) for fp in file_list]
    for f in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc="Processing files"):
        try:
            await f
            summary["files_processed"] += 1
        except Exception as e:
            summary["errors"] += 1
            logger.error(f"Error processing a file: {e}")
            logger.debug(traceback.format_exc())
    return summary

# --- Command-line Argument Parsing & Configurable Prompt ---
parser = argparse.ArgumentParser(
    description="Generate documentation for a project asynchronously with static analysis."
)
parser.add_argument(
    "--root", type=str, default=".", help="Root directory of the project (default: current directory)."
)
parser.add_argument(
    "--allowed_extensions", type=str, nargs="+",
    default=list(DEFAULT_ALLOWED_EXTENSIONS),
    help="List of allowed file extensions to process (default: .py .json .txt .md)."
)
parser.add_argument(
    "--prompt_config", type=str, default="",
    help="Path to a JSON file containing a custom prompt template. Should contain key 'base_prompt'."
)
parser.add_argument(
    "--base_prompt", type=str, default="",
    help="Directly override the base prompt via command-line (overrides prompt_config if provided)."
)
args = parser.parse_args()
args.allowed_extensions = {ext.lower() for ext in args.allowed_extensions}

# Load custom prompt if provided.
if args.base_prompt:
    BASE_PROMPT = args.base_prompt
elif args.prompt_config:
    try:
        with open(args.prompt_config, "r", encoding="utf-8") as f:
            config = json.load(f)
        BASE_PROMPT = config.get("base_prompt", DEFAULT_BASE_PROMPT)
    except Exception as e:
        logger.error(f"Failed to load prompt config: {e}")
        BASE_PROMPT = DEFAULT_BASE_PROMPT
else:
    BASE_PROMPT = DEFAULT_BASE_PROMPT

# --- Main Function ---
async def main():
    root_directory = pathlib.Path(args.root)
    logger.info(f"Starting documentation generation in: {root_directory}")
    summary = await iterate_project(root_directory)
    save_cache()
    logger.info("Documentation generation complete.")
    logger.info("Summary Report:")
    logger.info(f"  Folders processed: {summary['folders_processed']}")
    logger.info(f"  Files processed: {summary['files_processed']}")
    logger.info(f"  Errors encountered: {summary['errors']}")
    logger.info(f"  Total token usage: {total_token_usage}")

if __name__ == "__main__":
    asyncio.run(main())
